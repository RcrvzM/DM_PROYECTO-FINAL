{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOizGxNIdit5P8YjaHXeM0R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RcrvzM/DM_PROYECTO-FINAL/blob/main/Proyecciones_economicas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar Java y Spark\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Configurar variables de entorno\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "# Iniciar Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PIB_Mundial\").getOrCreate()\n",
        "\n",
        "spark.version\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_s1Y77uZ3Lkf",
        "outputId": "61b4b937-7db1-4681-db66-3a2dff9f8ac4"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jre\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-11-demo openjdk-11-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jdk\n",
            "  openjdk-11-jre x11-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 5,366 kB of archives.\n",
            "After this operation, 15.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.26+4-1ubuntu1~22.04 [214 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.26+4-1ubuntu1~22.04 [1,341 kB]\n",
            "Fetched 5,366 kB in 1s (4,698 kB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126332 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../1-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../2-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../3-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../4-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../5-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../6-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../7-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../8-openjdk-11-jre_11.0.26+4-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../9-openjdk-11-jdk_11.0.26+4-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "tar: spark-3.3.2-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fin de instalacion de Spark"
      ],
      "metadata": {
        "id": "oduXp8StE_FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Incio de descarga de fuentes de datos"
      ],
      "metadata": {
        "id": "zek3nOytFCe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json\n",
        "\n",
        "# API del Banco Mundial - PIB\n",
        "url_pib = \"http://api.worldbank.org/v2/country/all/indicator/NY.GDP.MKTP.CD?format=json&per_page=20000\"\n",
        "r_pib = requests.get(url_pib)\n",
        "data_pib = r_pib.json()[1]\n",
        "\n",
        "# Guardar como JSONL\n",
        "with open(\"/content/pib.json\", \"w\") as f:\n",
        "    for record in data_pib:\n",
        "        f.write(json.dumps(record) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "0W0VtwRa-_Ic"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API del Banco Mundial - Tasa de empleo (% de población)\n",
        "url_empleo = \"http://api.worldbank.org/v2/country/all/indicator/SL.EMP.TOTL.SP.ZS?format=json&per_page=20000\"\n",
        "r_empleo = requests.get(url_empleo)\n",
        "data_empleo = r_empleo.json()[1]\n",
        "\n",
        "# Guardar como JSONL\n",
        "with open(\"/content/empleo.json\", \"w\") as f:\n",
        "    for record in data_empleo:\n",
        "        f.write(json.dumps(record) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "bXD1yy3T_F7J"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# API SDMX del FMI – Tipo de cambio (ENDE_XDC_USD_RATE)\n",
        "url_fx = \"http://dataservices.imf.org/REST/SDMX_XML.svc/CompactData/IFS/A..ENDE_XDC_USD_RATE\"\n",
        "\n",
        "# Descargar archivo\n",
        "r_fx = requests.get(url_fx)\n",
        "\n",
        "# Guardar en disco (sin procesamiento aún)\n",
        "with open(\"/content/fx.xml\", \"wb\") as f:\n",
        "    f.write(r_fx.content)\n"
      ],
      "metadata": {
        "id": "6GbtlSrR_KNk"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fin de descarga de fuentes de datos"
      ],
      "metadata": {
        "id": "g_wdgXqkFFuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicio de validaciones de fuentes de datos"
      ],
      "metadata": {
        "id": "caF5a87SE0oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Cargar archivos JSONL\n",
        "df_pib = spark.read.json(\"/content/pib.json\")\n",
        "df_empleo = spark.read.json(\"/content/empleo.json\")\n",
        "\n",
        "# Función de validación de estructura\n",
        "def validar_dataset(df, nombre, columnas):\n",
        "    print(f\"\\n📊 Validando: {nombre}\")\n",
        "    errores = {}\n",
        "    total = df.count()\n",
        "    print(f\"Total filas: {total}\")\n",
        "\n",
        "    for columna in columnas:\n",
        "        nulos = df.filter(col(columna).isNull()).count()\n",
        "        errores[columna] = nulos\n",
        "        print(f\"Nulos en '{columna}': {nulos}\")\n",
        "\n",
        "    return errores\n",
        "\n",
        "# Validar PIB\n",
        "validar_dataset(df_pib, \"PIB\", [\"country.value\", \"date\", \"value\"])\n",
        "\n",
        "# Validar Empleo\n",
        "validar_dataset(df_empleo, \"Empleo\", [\"country.value\", \"date\", \"value\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv-DZeOZADoo",
        "outputId": "3e798c6f-2717-42a9-8a26-ffc84d49eec0"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Validando: PIB\n",
            "Total filas: 17290\n",
            "Nulos en 'country.value': 0\n",
            "Nulos en 'date': 0\n",
            "Nulos en 'value': 2983\n",
            "\n",
            "📊 Validando: Empleo\n",
            "Total filas: 17290\n",
            "Nulos en 'country.value': 0\n",
            "Nulos en 'date': 0\n",
            "Nulos en 'value': 9309\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'country.value': 0, 'date': 0, 'value': 9309}"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Parsear el XML sin namespace\n",
        "tree = ET.parse(\"/content/fx.xml\")\n",
        "root = tree.getroot()\n",
        "\n",
        "# Extraer nodos <Series>\n",
        "series_nodos = root.findall(\".//{http://dataservices.imf.org/compact/IFS}Series\")\n",
        "\n",
        "print(f\"📌 Total Series encontradas: {len(series_nodos)}\")\n",
        "\n",
        "# Extraer observaciones\n",
        "data = []\n",
        "\n",
        "for serie in series_nodos:\n",
        "    pais = serie.attrib.get(\"REF_AREA\")\n",
        "    for obs in serie.findall(\"{http://dataservices.imf.org/compact/IFS}Obs\"):\n",
        "        anio = obs.attrib.get(\"TIME_PERIOD\")\n",
        "        valor = obs.attrib.get(\"OBS_VALUE\")\n",
        "        if pais and anio and valor:\n",
        "            data.append((pais, int(anio), float(valor)))\n",
        "\n",
        "# Ver resumen\n",
        "print(f\"✅ Total registros válidos: {len(data)}\")\n",
        "print(\"Ejemplo:\", data[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NggBCy66EUzf",
        "outputId": "901d35a5-b63c-40d7-d8c1-754e769e2664"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📌 Total Series encontradas: 226\n",
            "✅ Total registros válidos: 13939\n",
            "Ejemplo: [('NG', 1950, 0.714286), ('NG', 1951, 0.714286), ('NG', 1952, 0.714286), ('NG', 1953, 0.714286), ('NG', 1954, 0.714286)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fin de validaciones de fuentes de datos"
      ],
      "metadata": {
        "id": "acxTz44_E4mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar SPARK"
      ],
      "metadata": {
        "id": "_X55rivYFeXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pib = spark.read.json(\"/content/pib.json\")\n",
        "\n",
        "# Filtrar registros útiles\n",
        "df_pib_limpio = df_pib.filter(\n",
        "    col(\"country.value\").isNotNull() &\n",
        "    col(\"date\").isNotNull() &\n",
        "    col(\"value\").isNotNull()\n",
        ")\n",
        "\n",
        "# Renombrar columnas y tipos\n",
        "df_pib_limpio = df_pib_limpio.select(\n",
        "    col(\"country.value\").alias(\"pais\"),\n",
        "    col(\"date\").cast(\"int\").alias(\"anio\"),\n",
        "    col(\"value\").cast(\"double\").alias(\"pib_usd\")\n",
        ")\n",
        "\n",
        "df_pib_limpio.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YElUZbemFf-d",
        "outputId": "995aceeb-0c45-4528-d98f-5bf1e1c15336"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+-------------------+\n",
            "|                pais|anio|            pib_usd|\n",
            "+--------------------+----+-------------------+\n",
            "|Africa Eastern an...|2023|1.24547247167595E12|\n",
            "|Africa Eastern an...|2022|1.19142317624296E12|\n",
            "|Africa Eastern an...|2021|1.08574517885097E12|\n",
            "|Africa Eastern an...|2020|9.33391782089617E11|\n",
            "|Africa Eastern an...|2019|1.00972117405491E12|\n",
            "+--------------------+----+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_empleo = spark.read.json(\"/content/empleo.json\")\n",
        "\n",
        "# Filtrar registros válidos\n",
        "df_empleo_limpio = df_empleo.filter(\n",
        "    col(\"country.value\").isNotNull() &\n",
        "    col(\"date\").isNotNull() &\n",
        "    col(\"value\").isNotNull()\n",
        ")\n",
        "\n",
        "# Renombrar columnas y tipos\n",
        "df_empleo_limpio = df_empleo_limpio.select(\n",
        "    col(\"country.value\").alias(\"pais\"),\n",
        "    col(\"date\").cast(\"int\").alias(\"anio\"),\n",
        "    col(\"value\").cast(\"double\").alias(\"tasa_empleo\")\n",
        ")\n",
        "\n",
        "df_empleo_limpio.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1NL8r7wFhD3",
        "outputId": "4c78e2a3-0684-4b30-a899-c3cb3a87e4bd"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+----------------+\n",
            "|                pais|anio|     tasa_empleo|\n",
            "+--------------------+----+----------------+\n",
            "|Africa Eastern an...|2024|63.8048907252938|\n",
            "|Africa Eastern an...|2023|63.8966859801838|\n",
            "|Africa Eastern an...|2022|61.6567788664629|\n",
            "|Africa Eastern an...|2021|61.0661154052549|\n",
            "|Africa Eastern an...|2020|60.8607715076156|\n",
            "+--------------------+----+----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Guardar la lista como archivo CSV\n",
        "with open(\"/content/fx.csv\", \"w\", newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"pais\", \"anio\", \"tipo_cambio\"])\n",
        "    writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "6PblZohkFlvh"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fx_limpio = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/content/fx.csv\")\n",
        "df_fx_limpio.show(5)\n",
        "df_fx_limpio.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LheovIWdF8Fv",
        "outputId": "10e2c5ed-abd3-4c7a-ec5c-0f3c0c651cee"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-----------+\n",
            "|pais|anio|tipo_cambio|\n",
            "+----+----+-----------+\n",
            "|  NG|1950|   0.714286|\n",
            "|  NG|1951|   0.714286|\n",
            "|  NG|1952|   0.714286|\n",
            "|  NG|1953|   0.714286|\n",
            "|  NG|1954|   0.714286|\n",
            "+----+----+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- pais: string (nullable = true)\n",
            " |-- anio: integer (nullable = true)\n",
            " |-- tipo_cambio: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga de Paises / Normalizacion"
      ],
      "metadata": {
        "id": "rczSG_OTF9WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_alt = \"https://raw.githubusercontent.com/datasets/country-codes/master/data/country-codes.csv\"\n",
        "r = requests.get(url_alt)\n",
        "\n",
        "with open(\"/content/paises_codigos.csv\", \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "df_codigos = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/content/paises_codigos.csv\")\n",
        "\n",
        "# Verificar columnas disponibles\n",
        "df_codigos.columns[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihMGhDoVGs9u",
        "outputId": "e2892b21-e065-456f-9fbd-77c6342f5523"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['FIFA',\n",
              " 'Dial',\n",
              " 'ISO3166-1-Alpha-3',\n",
              " 'MARC',\n",
              " 'is_independent',\n",
              " 'ISO3166-1-numeric',\n",
              " 'GAUL',\n",
              " 'FIPS',\n",
              " 'WMO',\n",
              " 'ISO3166-1-Alpha-2']"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar código y nombre\n",
        "df_codigos = df_codigos.select(\n",
        "    col(\"ISO3166-1-Alpha-2\").alias(\"pais_codigo\"),\n",
        "    col(\"official_name_en\").alias(\"pais\")\n",
        ").filter(col(\"pais\").isNotNull() & col(\"pais_codigo\").isNotNull())\n",
        "\n",
        "df_codigos.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXO79gk_HGqb",
        "outputId": "6c3c9f2e-4938-4adf-e159-249b43ddb297"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|pais_codigo|          pais|\n",
            "+-----------+--------------+\n",
            "|         AF|   Afghanistan|\n",
            "|         AX| Åland Islands|\n",
            "|         AL|       Albania|\n",
            "|         DZ|       Algeria|\n",
            "|         AS|American Samoa|\n",
            "+-----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_fx_final = df_fx_limpio.join(df_codigos, df_fx_limpio.pais == df_codigos.pais_codigo, \"left\") \\\n",
        "                           .select(\"pais\", \"anio\", \"tipo_cambio\")\n",
        "\n",
        "df_fx_final.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "0XyIiKr6HJ49",
        "outputId": "a6ad3b01-4d69-4726-ed3e-72369ba5e690"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Reference 'pais' is ambiguous, could be: pais, pais.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-0a882b4847c2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_fx_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_fx_limpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_codigos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_fx_limpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpais\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf_codigos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpais_codigo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m                            \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pais\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"anio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tipo_cambio\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_fx_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2021\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \"\"\"\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Reference 'pais' is ambiguous, could be: pais, pais."
          ]
        }
      ]
    }
  ]
}